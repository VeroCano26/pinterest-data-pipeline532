{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa975c6a-7e73-464a-86b8-87bd340a1917",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936d3474-2017-4f04-a87e-79b2776a608f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------------------+--------------------+---------------+\n",
       "             country|         poster_name| follower_count|\n",
       "+--------------------+--------------------+---------------+\n",
       "         Afghanistan|          AllPosters|            72k|\n",
       "             Albania|     User Info Error|User Info Error|\n",
       "             Algeria|           YourTango|           942k|\n",
       "      American Samoa|         Mamas Uncut|             8M|\n",
       "             Andorra|           Glaminati|           799k|\n",
       "              Angola|           Tastemade|             8M|\n",
       "            Anguilla|Kristen | Lifesty...|            92k|\n",
       "Antarctica (the t...|            HikenDip|           501k|\n",
       " Antigua and Barbuda|        rhonda_floyd|             3k|\n",
       "           Argentina|         Next Luxury|           800k|\n",
       "             Armenia|No Hurry To Get Home|             9k|\n",
       "             Armenia|        Pacho Tattoo|             9k|\n",
       "               Aruba|    Linda On The Run|             9k|\n",
       "           Australia|Write Your Story ...|             5k|\n",
       "             Austria|The World Pursuit...|            89k|\n",
       "          Azerbaijan|     Style Me Pretty|             6M|\n",
       "             Bahamas|       Andrew Martin|             5k|\n",
       "             Bahrain|R.J. Weiss at The...|            46k|\n",
       "          Bangladesh|Better Homes and ...|             4M|\n",
       "            Barbados|The Creativity Ex...|           410k|\n",
       "+--------------------+--------------------+---------------+\n",
       "only showing top 20 rows\n",
       "\n",
       "+-------+---------------+\n",
       "country| follower_count|\n",
       "+-------+---------------+\n",
       "Albania|User Info Error|\n",
       "+-------+---------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+--------------------+--------------------+---------------+\n|             country|         poster_name| follower_count|\n+--------------------+--------------------+---------------+\n|         Afghanistan|          AllPosters|            72k|\n|             Albania|     User Info Error|User Info Error|\n|             Algeria|           YourTango|           942k|\n|      American Samoa|         Mamas Uncut|             8M|\n|             Andorra|           Glaminati|           799k|\n|              Angola|           Tastemade|             8M|\n|            Anguilla|Kristen | Lifesty...|            92k|\n|Antarctica (the t...|            HikenDip|           501k|\n| Antigua and Barbuda|        rhonda_floyd|             3k|\n|           Argentina|         Next Luxury|           800k|\n|             Armenia|No Hurry To Get Home|             9k|\n|             Armenia|        Pacho Tattoo|             9k|\n|               Aruba|    Linda On The Run|             9k|\n|           Australia|Write Your Story ...|             5k|\n|             Austria|The World Pursuit...|            89k|\n|          Azerbaijan|     Style Me Pretty|             6M|\n|             Bahamas|       Andrew Martin|             5k|\n|             Bahrain|R.J. Weiss at The...|            46k|\n|          Bangladesh|Better Homes and ...|             4M|\n|            Barbados|The Creativity Ex...|           410k|\n+--------------------+--------------------+---------------+\nonly showing top 20 rows\n\n+-------+---------------+\n|country| follower_count|\n+-------+---------------+\n|Albania|User Info Error|\n+-------+---------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the paths for Pinterest post, geolocation, and user data in your S3 bucket\n",
    "pin_data_path = \"/mnt/0e6999790cc9_mount_name/topics/0e6999790cc9.pin/partition=0/*.json\"\n",
    "geo_data_path = \"/mnt/0e6999790cc9_mount_name/topics/0e6999790cc9.geo/partition=0/*.json\"\n",
    "user_data_path = \"/mnt/0e6999790cc9_mount_name/topics/0e6999790cc9.user/partition=0/*.json\"\n",
    "\n",
    "# Read JSON files into DataFrames\n",
    "df_pin = spark.read.json(pin_data_path)\n",
    "df_geo = spark.read.json(geo_data_path)\n",
    "df_user = spark.read.json(user_data_path)\n",
    "\n",
    "# Replace empty entries and entries with no relevant data with Nones\n",
    "df_pin_cleaned = df_pin.na.fill(\"None\")\n",
    "\n",
    "# Perform transformations on the follower_count column\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\", df_pin_cleaned[\"follower_count\"].cast(IntegerType()))\n",
    "\n",
    "# Ensure that each column containing numeric data has a numeric data type\n",
    "numeric_columns = [\"follower_count\"]  \n",
    "for col in numeric_columns:\n",
    "    df_pin_cleaned = df_pin_cleaned.withColumn(col, df_pin_cleaned[col].cast(IntegerType()))\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"save_location\", F.expr(\"substring_index(save_location, '/', -1)\"))\n",
    "\n",
    "# Rename the index column to ind\n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "desired_column_order = [\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\",\n",
    "                        \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"]\n",
    "df_pin_cleaned = df_pin_cleaned.select(desired_column_order)\n",
    "\n",
    "# Show the cleaned and transformed DataFrame\n",
    "df_pin_cleaned.show()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, TimestampType\n",
    "\n",
    "# Create a new column 'coordinates' based on 'latitude' and 'longitude'\n",
    "df_geo = df_geo.withColumn('coordinates', F.array('latitude', 'longitude'))\n",
    "\n",
    "# Drop 'latitude' and 'longitude' columns\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "\n",
    "# Convert 'timestamp' column from string to timestamp data type\n",
    "df_geo = df_geo.withColumn('timestamp', F.to_timestamp('timestamp'))\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "desired_column_order_geo = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "df_geo = df_geo.select(desired_column_order_geo)\n",
    "\n",
    "# Show the cleaned and transformed DataFrame\n",
    "df_geo.show()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Create a new column user_name by concatenating first_name and last_name\n",
    "df_user = df_user.withColumn(\"user_name\", F.concat_ws(\" \", \"first_name\", \"last_name\"))\n",
    "\n",
    "# Drop the first_name and last_name columns\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert the date_joined column from string to timestamp\n",
    "df_user = df_user.withColumn(\"date_joined\", F.to_timestamp(\"date_joined\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "desired_column_order_user = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "df_user = df_user.select(desired_column_order_user)\n",
    "\n",
    "# Show the cleaned and transformed DataFrame\n",
    "df_user.show()\n",
    "\n",
    "# 1. Find the most popular category people post to based on their country.\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Join df_pin_cleaned and df_geo on the 'ind' column\n",
    "df_combined = df_pin_cleaned.join(df_geo, 'ind', 'inner')\n",
    "\n",
    "# Group by country and category, count the occurrences, and create a new column 'category_count'\n",
    "df_category_count = df_combined.groupBy('country', 'category').agg(F.count('ind').alias('category_count'))\n",
    "\n",
    "# Use Window function to rank categories within each country based on count\n",
    "window_spec = Window.partitionBy('country').orderBy(F.desc('category_count'))\n",
    "df_category_ranked = df_category_count.withColumn('rank', F.rank().over(window_spec))\n",
    "\n",
    "# Filter for the top-ranked category within each country\n",
    "df_most_popular_category = df_category_ranked.filter(F.col('rank') == 1).drop('rank')\n",
    "\n",
    "# Show the result\n",
    "df_most_popular_category.show()\n",
    "\n",
    "# 2. Find which was the most popular category each year\n",
    "from pyspark.sql.functions import col, year, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Join df_pin_cleaned and df_geo on the 'ind' column\n",
    "df_combined = df_pin_cleaned.join(df_geo, 'ind', 'inner')\n",
    "\n",
    "# Create a new column 'post_year' based on the 'timestamp' column\n",
    "df_combined = df_combined.withColumn(\"post_year\", year(\"timestamp\"))\n",
    "\n",
    "# Filter posts between 2018 and 2022\n",
    "df_filtered_posts = df_combined.filter((col(\"post_year\") >= 2018) & (col(\"post_year\") <= 2022))\n",
    "\n",
    "# Group by post_year, category, and count the occurrences\n",
    "df_category_count = df_filtered_posts.groupBy(\"post_year\", \"category\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# Show the result\n",
    "df_category_count.show()\n",
    "\n",
    "# 3. Find the user with the most followers in each country\n",
    "\n",
    "from pyspark.sql.functions import max, rank, desc\n",
    "\n",
    "# Find the user with the most followers in each country\n",
    "df_most_followers_by_country = df_pin.join(df_geo, df_pin[\"index\"] == df_geo[\"ind\"]) \\\n",
    "    .groupBy('country', 'poster_name') \\\n",
    "    .agg(max('follower_count')) \\\n",
    "    .withColumn('rank', rank().over(Window.partitionBy('country').orderBy(desc('max(follower_count)')))) \\\n",
    "    .filter('rank = 1') \\\n",
    "    .select('country', 'poster_name', 'max(follower_count)') \\\n",
    "    .withColumnRenamed('max(follower_count)', 'follower_count')\n",
    "\n",
    "df_most_followers_by_country.show()\n",
    "\n",
    "# Find the country with the user that has the most followers\n",
    "df_most_followers_country = df_most_followers_by_country.groupBy('country') \\\n",
    "    .agg(max('follower_count').alias('follower_count')) \\\n",
    "    .orderBy(desc('follower_count')) \\\n",
    "    .limit(1) \\\n",
    "    .select('country', 'follower_count')\n",
    "\n",
    "df_most_followers_country.show()\n",
    "\n",
    "# 4. Find the most popular category for different age groups\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    '18-24': (18, 24),\n",
    "    '25-35': (25, 35),\n",
    "    '36-50': (36, 50),\n",
    "    '+50': (51, float('inf'))  # Assuming 50 and above as '+50'\n",
    "}\n",
    "\n",
    "# Create a new column 'age_group' based on the 'age' column\n",
    "df_user_with_age_group = df_user.withColumn(\n",
    "    'age_group',\n",
    "    when((col('age') >= age_groups['18-24'][0]) & (col('age') <= age_groups['18-24'][1]), '18-24')\n",
    "    .when((col('age') >= age_groups['25-35'][0]) & (col('age') <= age_groups['25-35'][1]), '25-35')\n",
    "    .when((col('age') >= age_groups['36-50'][0]) & (col('age') <= age_groups['36-50'][1]), '36-50')\n",
    "    .when((col('age') >= age_groups['+50'][0]), '+50')\n",
    "    .otherwise('Unknown')\n",
    ")\n",
    "\n",
    "# Join df_pin_cleaned and df_geo on the 'ind' column\n",
    "df_combined = df_pin_cleaned.join(df_geo, 'ind', 'inner')\n",
    "\n",
    "# Join the combined DataFrame with df_user_with_age_group on the 'ind' column\n",
    "df_combined_with_age_group = df_combined.join(df_user_with_age_group, 'ind', 'inner')\n",
    "\n",
    "# Group by age_group, category, and count the occurrences\n",
    "df_category_count_by_age_group = df_combined_with_age_group.groupBy('age_group', 'category').agg(count('*').alias('category_count'))\n",
    "\n",
    "# Show the result\n",
    "df_category_count_by_age_group.show()\n",
    "\n",
    "# 5. Find the median follower count for different age groups\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the age groups\n",
    "age_groups = [\"18-24\", \"25-35\", \"36-50\", \"+50\"]\n",
    "\n",
    "# Create a DataFrame with the age groups\n",
    "df_age_groups = spark.createDataFrame([(group,) for group in age_groups], [\"age_group\"])\n",
    "\n",
    "# Define conditions for age groups\n",
    "conditions = [\n",
    "    (F.col(\"age\") >= 18) & (F.col(\"age\") <= 24),\n",
    "    (F.col(\"age\") >= 25) & (F.col(\"age\") <= 35),\n",
    "    (F.col(\"age\") >= 36) & (F.col(\"age\") <= 50),\n",
    "    (F.col(\"age\") > 50)\n",
    "]\n",
    "\n",
    "# Use the conditions to create the age_group column\n",
    "df_user = df_user.withColumn(\"age_group\", F.when(conditions[0], \"18-24\")\n",
    "                                       .when(conditions[1], \"25-35\")\n",
    "                                       .when(conditions[2], \"36-50\")\n",
    "                                       .when(conditions[3], \"+50\")\n",
    "                                       .otherwise(None))\n",
    "\n",
    "# Join df_user and df_pin_cleaned on a common column, e.g., ind\n",
    "df_user_with_follower_count = df_user.join(df_pin_cleaned, \"ind\", \"inner\")\n",
    "\n",
    "# Join df_user_with_follower_count and df_age_groups to include age_group information\n",
    "df_user_with_follower_and_age = df_user_with_follower_count.join(df_age_groups, \"age_group\", \"right_outer\")\n",
    "\n",
    "# Calculate the median follower count for each age group\n",
    "df_median_follower_count = df_user_with_follower_and_age.groupBy(\"age_group\") \\\n",
    "    .agg(F.expr(\"percentile(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "\n",
    "# Show the result\n",
    "df_median_follower_count.show()\n",
    "\n",
    "# 6. Find how many users have joined each year\n",
    "\n",
    "from pyspark.sql.functions import year, count\n",
    "\n",
    "# Create a new column 'post_year' based on the 'date_joined' column\n",
    "df_user_joined = df_user.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "\n",
    "# Filter users who joined between 2015 and 2020\n",
    "df_filtered_users = df_user_joined.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Group by post_year and count the occurrences\n",
    "df_users_joined_count = df_filtered_users.groupBy(\"post_year\").agg(count(\"*\").alias(\"number_users_joined\"))\n",
    "\n",
    "# Show the result\n",
    "df_users_joined_count.show()\n",
    "\n",
    "# 7. Find the median follower count of users based on their joining year\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming df_user_with_follower_and_age is the DataFrame containing user information\n",
    "# Make sure to replace it with the actual DataFrame you have\n",
    "\n",
    "# Create the post_year column\n",
    "df_filtered_users = df_user_with_follower_and_age.withColumn(\"post_year\", F.year(\"date_joined\"))\n",
    "\n",
    "# Filter users who joined between 2015 and 2020\n",
    "df_filtered_users = df_filtered_users.filter((F.col(\"post_year\") >= 2015) & (F.col(\"post_year\") <= 2020))\n",
    "\n",
    "# Calculate the median follower count for each post_year\n",
    "df_median_follower_count = df_filtered_users.groupBy(\"post_year\") \\\n",
    "    .agg(F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "\n",
    "df_median_follower_count.show()\n",
    "\n",
    "# 8. Find the median follow count of users based on their joining year and age group\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming df_user_with_follower_and_age is the DataFrame containing user information\n",
    "# Make sure to replace it with the actual DataFrame you have\n",
    "\n",
    "# Create the post_year column\n",
    "df_filtered_users = df_user_with_follower_and_age.withColumn(\"post_year\", F.year(\"date_joined\"))\n",
    "\n",
    "# Filter users who joined between 2015 and 2020\n",
    "df_filtered_users = df_filtered_users.filter((F.col(\"post_year\") >= 2015) & (F.col(\"post_year\") <= 2020))\n",
    "\n",
    "# Calculate the median follower count for each age group and post_year\n",
    "df_median_follower_count_by_age_group = df_filtered_users.groupBy(\"age_group\", \"post_year\") \\\n",
    "    .agg(F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "\n",
    "df_median_follower_count_by_age_group.show()\n",
    "\n",
    "\n",
    "# Display the schema of df_pin\n",
    "print(\"Schema of df_pin:\")\n",
    "df_pin.printSchema()\n",
    "\n",
    "# Display the schema of df_geo\n",
    "print(\"Schema of df_geo:\")\n",
    "df_geo.printSchema()\n",
    "\n",
    "df_user_joined.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_and_query_batch_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
