{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0a49c5-dd8d-4dad-ae2f-aa7299301564",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ind</th><th>timestamp</th><th>latitude</th><th>longitude</th><th>country</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ind",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "latitude",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "longitude",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, FloatType\n",
    "\n",
    "# Define a function to read AWS credentials from Delta table\n",
    "def read_aws_credentials_from_delta(delta_table_path):\n",
    "    aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "    ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "    SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "    ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "    return ACCESS_KEY, SECRET_KEY\n",
    "\n",
    "# Define a function to read AWS credentials from CSV file\n",
    "def read_aws_credentials_from_csv(csv_file_path):\n",
    "    file_type = \"csv\"\n",
    "    first_row_is_header = \"true\"\n",
    "    delimiter = \",\"\n",
    "    \n",
    "    csv_schema = StructType([\n",
    "        StructField(\"User name\", StringType()),\n",
    "        StructField(\"Access key ID\", StringType()),\n",
    "        StructField(\"Secret access key\", StringType())\n",
    "    ])\n",
    "\n",
    "    aws_keys_df = spark.read.format(file_type)\\\n",
    "        .option(\"header\", first_row_is_header)\\\n",
    "        .option(\"sep\", delimiter)\\\n",
    "        .schema(csv_schema)\\\n",
    "        .load(csv_file_path)\n",
    "\n",
    "    ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "    SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "    ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "    return ACCESS_KEY, SECRET_KEY\n",
    "\n",
    "# Choose either Delta table or CSV file method based on where your credentials are stored\n",
    "use_delta_table = True  # Set this to True if using Delta table, False if using CSV file\n",
    "\n",
    "if use_delta_table:\n",
    "    # Read AWS credentials from Delta table\n",
    "    delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "    try:\n",
    "        ACCESS_KEY, SECRET_KEY = read_aws_credentials_from_delta(delta_table_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading AWS credentials from Delta table: {str(e)}\")\n",
    "        # Handle the error as needed, e.g., exit the script or set default values\n",
    "else:\n",
    "    # Read AWS credentials from CSV file\n",
    "    csv_file_path = \"/FileStore/tables/authentication_credentials.csv\"\n",
    "    try:\n",
    "        ACCESS_KEY, SECRET_KEY = read_aws_credentials_from_csv(csv_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading AWS credentials from CSV file: {str(e)}\")\n",
    "        # Handle the error as needed, e.g., exit the script or set default values\n",
    "\n",
    "# Define a function to read data from Kinesis stream using structured streaming\n",
    "def read_kinesis_stream(stream_name, region_name, schema):\n",
    "    return spark.readStream \\\n",
    "        .format(\"kinesis\") \\\n",
    "        .option(\"streamName\", stream_name) \\\n",
    "        .option(\"regionName\", region_name) \\\n",
    "        .option(\"initialPosition\", \"TRIM_HORIZON\") \\\n",
    "        .option(\"format\", \"json\") \\\n",
    "        .option(\"awsAccessKey\", ACCESS_KEY) \\\n",
    "        .option(\"awsSecretKey\", SECRET_KEY) \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load() \\\n",
    "        .withColumn(\"cast_data\", from_json(col(\"data\").cast(\"string\"), schema)) \\\n",
    "        .select(\"cast_data.*\")\n",
    "\n",
    "# Define the schema for the pin stream\n",
    "pin_schema = StructType([\n",
    "    StructField(\"index\", IntegerType()),\n",
    "    StructField(\"unique_id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"poster_name\", StringType()),\n",
    "    StructField(\"follower_count\", StringType()),\n",
    "    StructField(\"tag_list\", StringType()),\n",
    "    StructField(\"is_image_or_video\", StringType()),\n",
    "    StructField(\"image_src\", StringType()),\n",
    "    StructField(\"downloaded\", IntegerType()),\n",
    "    StructField(\"save_location\", StringType()),\n",
    "    StructField(\"category\", StringType())\n",
    "])\n",
    "\n",
    "# Define the schema for the geo stream\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"latitude\", FloatType()),\n",
    "    StructField(\"longitude\", FloatType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "# Define the schema for the user stream\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"date_joined\", TimestampType())\n",
    "])\n",
    "\n",
    "# Read data from Kinesis streams\n",
    "streaming_df_pin = read_kinesis_stream(\"streaming-0e6999790cc9-pin\", \"us-east-1\", pin_schema)\n",
    "streaming_df_geo = read_kinesis_stream(\"streaming-0e6999790cc9-geo\", \"us-east-1\", geo_schema)\n",
    "streaming_df_user = read_kinesis_stream(\"streaming-0e6999790cc9-user\", \"us-east-1\", user_schema)\n",
    "\n",
    "# Display the DataFrames\n",
    "display(streaming_df_pin)\n",
    "display(streaming_df_geo)\n",
    "display(streaming_df_user)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2851bb-4779-4921-8279-f4c8a1b117e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ind</th><th>user_name</th><th>age</th><th>date_joined</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ind",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "user_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date_joined",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning and Transforming the Pin Stream:\n",
    "from pyspark.sql.functions import regexp_replace, when, col, lit\n",
    "\n",
    "# Cleaning the description column\n",
    "streaming_df_pin = streaming_df_pin.withColumn(\n",
    "    \"description\",\n",
    "    when(\n",
    "        (streaming_df_pin.description.isin(\"Untitled\", \"No description available\", \"No description available Story format\")),\n",
    "        \"None\"\n",
    "    ).otherwise(streaming_df_pin.description)\n",
    ")\n",
    "\n",
    "# Cleaning the tag_list column\n",
    "streaming_df_pin = streaming_df_pin.withColumn(\n",
    "    \"tag_list\",\n",
    "    when(\n",
    "        (streaming_df_pin.tag_list.isin(\"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", \"0\")),\n",
    "        \"None\"\n",
    "    ).otherwise(streaming_df_pin.tag_list)\n",
    ")\n",
    "\n",
    "# Cleaning the title column\n",
    "streaming_df_pin = streaming_df_pin.withColumn(\n",
    "    \"title\",\n",
    "    when(\n",
    "        (streaming_df_pin.title == \"No Title Data Available\"),\n",
    "        \"None\"\n",
    "    ).otherwise(streaming_df_pin.title)\n",
    ")\n",
    "\n",
    "# Cleaning the follower_count column\n",
    "streaming_df_pin = streaming_df_pin.withColumn(\n",
    "    \"follower_count\",\n",
    "    when(\n",
    "        (streaming_df_pin.follower_count == \"User Info Error\"),\n",
    "        \"0\"\n",
    "    ).otherwise(streaming_df_pin.follower_count)\n",
    ")\n",
    "\n",
    "# Cleaning the image_src column\n",
    "streaming_df_pin = streaming_df_pin.withColumn(\n",
    "    \"image_src\",\n",
    "    when(\n",
    "        (streaming_df_pin.image_src == \"Image src error.\"),\n",
    "        \"None\"\n",
    "    ).otherwise(streaming_df_pin.image_src)\n",
    ")\n",
    "\n",
    "# Cleaning the poster_name column\n",
    "streaming_df_pin = streaming_df_pin.withColumn(\n",
    "    \"poster_name\",\n",
    "    when(\n",
    "        (streaming_df_pin.poster_name == \"User Info Error\"),\n",
    "        \"None\"\n",
    "    ).otherwise(streaming_df_pin.poster_name)\n",
    ")\n",
    "\n",
    "# Cleaning the downloaded column\n",
    "streaming_df_pin = streaming_df_pin.withColumn(\n",
    "    \"downloaded\",\n",
    "    when(\n",
    "        (streaming_df_pin.downloaded == \"None\"),\n",
    "        \"0\"\n",
    "    ).otherwise(streaming_df_pin.downloaded)\n",
    ")\n",
    "\n",
    "# Replace 'k' and 'M' in follower_count, and change datatype to int\n",
    "streaming_df_pin = streaming_df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\")) \\\n",
    "    .withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\")) \\\n",
    "    .withColumn(\"follower_count\", col(\"follower_count\").cast(\"int\"))\n",
    "\n",
    "# Rename 'index' to 'ind'\n",
    "streaming_df_pin = streaming_df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Remove \"Local save in\" from save_location\n",
    "streaming_df_pin = streaming_df_pin.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "\n",
    "# Select the desired columns\n",
    "streaming_df_pin = streaming_df_pin.select(\n",
    "    col(\"ind\"), col(\"unique_id\"), col(\"title\"), col(\"description\"),\n",
    "    col(\"follower_count\"), col(\"poster_name\"), col(\"tag_list\"),\n",
    "    col(\"is_image_or_video\"), col(\"image_src\"), col(\"save_location\"), col(\"category\")\n",
    ")\n",
    "\n",
    "display(streaming_df_pin)\n",
    "\n",
    "\n",
    "# Cleaning and Transforming the Geo Stream:\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "# Combine latitude and longitude into a coordinates array\n",
    "streaming_df_geo = streaming_df_geo.withColumn(\"coordinates\", array(\"latitude\", \"longitude\")).drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Change the datatype of the \"timestamp\" column to timestamp\n",
    "streaming_df_geo = streaming_df_geo.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Select the desired columns\n",
    "streaming_df_geo = streaming_df_geo.select(col(\"ind\"), col(\"country\"), col(\"coordinates\"), col(\"timestamp\"))\n",
    "\n",
    "display(streaming_df_geo)\n",
    "\n",
    "# Cleaning and Transforming the User Stream:\n",
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "# Combine first_name and last_name into a user_name column\n",
    "streaming_df_user = streaming_df_user.withColumn('user_name', concat(streaming_df_user.first_name, lit(' '), streaming_df_user.last_name)).drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Change the datatype of the \"date_joined\" column to timestamp\n",
    "streaming_df_user = streaming_df_user.withColumn(\"date_joined\", col(\"date_joined\").cast(\"timestamp\"))\n",
    "\n",
    "# Select the desired columns\n",
    "streaming_df_user = streaming_df_user.select(col(\"ind\"), col(\"user_name\"), col(\"age\"), col(\"date_joined\"))\n",
    "\n",
    "display(streaming_df_user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b97b2b7-4148-41cf-9a64-e5487b4d6f95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ind</th><th>unique_id</th><th>title</th><th>description</th><th>follower_count</th><th>poster_name</th><th>tag_list</th><th>is_image_or_video</th><th>image_src</th><th>save_location</th><th>category</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ind",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "unique_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "follower_count",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "poster_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tag_list",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_image_or_video",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "image_src",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "save_location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ind</th><th>country</th><th>coordinates</th><th>timestamp</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ind",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "coordinates",
         "type": "{\"type\":\"array\",\"elementType\":\"float\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ind</th><th>user_name</th><th>age</th><th>date_joined</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ind",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "user_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date_joined",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Save the cleaned and transformed streaming DataFrames as Delta Tables\n",
    "\n",
    "# Save the Pin Stream\n",
    "streaming_df_pin.writeStream.format(\"memory\").queryName(\"pin_temp_table\").start()\n",
    "\n",
    "# Read the temporary table as a non-streaming DataFrame\n",
    "not_streaming_df_pin = spark.sql(\"SELECT * FROM pin_temp_table\")\n",
    "\n",
    "# Save the non-streaming DataFrame as a Delta Table\n",
    "not_streaming_df_pin.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"0e6999790cc9_pin_table\")\n",
    "\n",
    "# Display the non-streaming DataFrame\n",
    "display(not_streaming_df_pin)\n",
    "\n",
    "# Save the Geo Stream\n",
    "streaming_df_geo.writeStream.format(\"memory\").queryName(\"geo_temp_table\").start()\n",
    "\n",
    "# Read the temporary table as a non-streaming DataFrame\n",
    "not_streaming_df_geo = spark.sql(\"SELECT * FROM geo_temp_table\")\n",
    "\n",
    "# Save the non-streaming DataFrame as a Delta Table\n",
    "not_streaming_df_geo.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"0e6999790cc9_geo_table\")\n",
    "\n",
    "# Display the non-streaming DataFrame\n",
    "display(not_streaming_df_geo)\n",
    "\n",
    "# Save the User Stream\n",
    "streaming_df_user.writeStream.format(\"memory\").queryName(\"user_temp_table\").start()\n",
    "\n",
    "# Read the temporary table as a non-streaming DataFrame\n",
    "not_streaming_df_user = spark.sql(\"SELECT * FROM user_temp_table\")\n",
    "\n",
    "# Save the non-streaming DataFrame as a Delta Table\n",
    "not_streaming_df_user.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"0e6999790cc9_user_table\")\n",
    "\n",
    "# Display the non-streaming DataFrame\n",
    "display(not_streaming_df_user)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transforming_and_querying_the_streaming_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
